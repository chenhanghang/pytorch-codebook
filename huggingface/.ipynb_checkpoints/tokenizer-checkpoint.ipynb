{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fde40184-94ed-4426-91d4-36d8697156ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd056ceb-b032-44b4-b560-3c8451276326",
   "metadata": {},
   "source": [
    "## 1.用法示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026fe828-5ba2-45bf-a2b5-3a8851671620",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_senteces = ['today is not that bad', 'today is so bad', 'so good']\n",
    "batch_input = tokenizer(test_senteces, truncation=True, padding=True, return_tensors='pt')\n",
    "# >>> {'input_ids': tensor([[ 101, 2651, 2003, 2025, 2008, 2919,  102],[ 101, 2651, 2003, 2061, 2919,  102,    0]]),\n",
    "# >>> 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],[1, 1, 1, 1, 1, 1, 0]])}\n",
    "\n",
    "tokenizer(test_senteces[0], )\n",
    "# >>> {'input_ids': [101, 2651, 2003, 2025, 2008, 2919, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
    "\n",
    "tokenizer.encode(test_senteces[0], )\n",
    "# >>> [101, 2651, 2003, 2025, 2008, 2919, 102]\n",
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(test_senteces[0])) # 等价于 encode\n",
    "# >>> [2651, 2003, 2025, 2008, 2919]\n",
    "tokenizer.decode([101, 2651, 2003, 2025, 2008, 2919, 102])\n",
    "# >>> '[CLS] today is not that bad [SEP]'\n",
    "\n",
    "tokenizer.special_tokens_map.values() # 特殊字符\n",
    "# >>> dict_values(['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]'])\n",
    "tokenizer.convert_tokens_to_ids([special for special in tokenizer.special_tokens_map.values()])\n",
    "# >>> [100, 102, 0, 101, 103]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addbe27a-302b-4333-9657-a57740a04971",
   "metadata": {},
   "source": [
    "## 2.模型调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dcc1f0-f1b8-4fec-aaf3-ab1e51f4fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de18d992-4716-4c78-984c-6933913ac392",
   "metadata": {},
   "source": [
    "DistilBertConfig {\n",
    "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "  \"activation\": \"gelu\",\n",
    "  \"architectures\": [\n",
    "    \"DistilBertForSequenceClassification\"\n",
    "  ],\n",
    "  \"attention_dropout\": 0.1,\n",
    "  \"dim\": 768,\n",
    "  \"dropout\": 0.1,\n",
    "  \"finetuning_task\": \"sst-2\",\n",
    "  \"hidden_dim\": 3072,\n",
    "  \"id2label\": {\n",
    "    \"0\": \"NEGATIVE\",\n",
    "    \"1\": \"POSITIVE\"\n",
    "  },\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"label2id\": {\n",
    "    \"NEGATIVE\": 0,\n",
    "    \"POSITIVE\": 1\n",
    "  },\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"model_type\": \"distilbert\",\n",
    "  \"n_heads\": 12,\n",
    "  \"n_layers\": 6,\n",
    "  \"output_past\": true,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"qa_dropout\": 0.1,\n",
    "  \"seq_classif_dropout\": 0.2,\n",
    "  \"sinusoidal_pos_embds\": false,\n",
    "  \"tie_weights_\": true,\n",
    "  \"transformers_version\": \"4.11.2\",\n",
    "  \"vocab_size\": 30522\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa09fc6-b2b8-4d91-bbba-146e3a21a25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出解析\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch_input)\n",
    "    print(outputs)\n",
    "    scores = F.softmax(outputs.logits, dim=1)\n",
    "    print(scores)\n",
    "    labels = torch.argmax(scores, dim=1)\n",
    "    print(labels)\n",
    "    labels = [model.config.id2label[id] for id in labels.tolist()] # tolist 转成list\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e119d42-ee66-43a6-babc-bd6c69c633b8",
   "metadata": {},
   "source": [
    "SequenceClassifierOutput(loss=None, logits=tensor([[-3.4620,  3.6118],\n",
    "        [ 4.7508, -3.7899],\n",
    "        [-4.1938,  4.5566]]), hidden_states=None, attentions=None)\n",
    "tensor([[8.4632e-04, 9.9915e-01],\n",
    "        [9.9980e-01, 1.9531e-04],\n",
    "        [1.5837e-04, 9.9984e-01]])\n",
    "tensor([1, 0, 1])\n",
    "['POSITIVE', 'NEGATIVE', 'POSITIVE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7380c51-1551-4ba9-8bd8-392cdb055ae7",
   "metadata": {},
   "source": [
    "## 3.Bert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ff61b2-68e3-4e7c-85a7-947d4439a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# >>> PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right',\n",
    "# >>> special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
    "\n",
    "tokenizer.special_tokens_map\n",
    "# >>> {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3361502-73d9-4114-b7ec-2e3f059fb5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single sentence 级别的\n",
    "tokenizer(test_news[0], truncation=True, max_length=32)\n",
    "\n",
    "# >>> {'input_ids': [101, 2013, 1024, 3393, 2099, 2595, 3367, 1030, 11333, 2213, 1012, 8529, 2094, 1012, 3968, 2226, 1006, 2073, 1005, 1055, 2026, 2518, 1007, 3395, 1024, 2054, 2482, 2003, 2023, 999, 1029, 102], \n",
    "# >>>  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "# >>>  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
    "\n",
    "# sentence pair 级别\n",
    "tokenizer.encode_plus(text=test_news[0], text_pair=test_news[1], max_length=32, truncation=True)\n",
    "\n",
    "# >>> {'input_ids': [101, 2013, 1024, 3393, 2099, 2595, 3367, 1030, 11333, 2213, 1012, 8529, 2094, 1012, 3968, 2226, 102, 2013, 1024, 3124, 5283, 2080, 1030, 9806, 1012, 1057, 1012, 2899, 1012, 3968, 2226, 102], \n",
    "# >>> 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \n",
    "# >>> 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
